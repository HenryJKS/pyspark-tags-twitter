{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d3fab3-878e-41e1-ab49-02e1edb65d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1065b8-23ec-4d2b-a7c1-d1131321418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65df8cf4-7dc5-4ffa-8d68-ae94f0c4afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72ebd47d-3b13-4094-be83-5a5a6cafc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1722ea41-c453-45c1-8588-37ce478780f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a24474-e5d4-49e9-ac12-c49dae423631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Spark\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\streaming\\context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instance StreamingContext with 10s\n",
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b28cef-2505-43cd-808f-436109141b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 9998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666fdea5-cd8e-40ef-b34a-1e54b431fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = socket_stream.window(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca661f12-b7de-4c52-8579-301e8d7bfa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "fields = (\"tag\", \"count\")\n",
    "Tweet = namedtuple(\"Tweet\", field_names=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b656b25a-b050-4853-8cb1-369d2c1e240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(lines.flatMap(lambda text: text.split(\" \"))  # Divide o texto em palavras\n",
    "  .filter(lambda word: word.lower().startswith(\"#\"))  # Filtra apenas hashtags\n",
    "  .map(lambda word: (word.lower(), 1))  # Converte para minúsculas e adiciona contagem\n",
    "  .reduceByKey(lambda a, b: a + b)  # Soma as ocorrências\n",
    "  .map(lambda rec: Tweet(rec[0], rec[1]))  # Converte para um objeto Tweet\n",
    "  .foreachRDD(lambda rdd: rdd.toDF().sort(desc(\"count\"))  # Converte para DataFrame e ordena\n",
    "              .limit(10).createOrReplaceTempView(\"tweets\"))  # Cria a tabela temporária\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd5636ae-c428-4255-8d1c-44615aa2b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TweetRead running\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4846d219-b9d8-44b3-b08f-5f63bbd8a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e955a59-175f-4099-bb53-4eebc0751219",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `tweets` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 23;\n'Project ['tag, 'count]\n+- 'UnresolvedRelation [tweets], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m count \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m      3\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     top_10_tweets \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT tag, count FROM tweets\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Usando SparkSession\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     top_10_df \u001b[38;5;241m=\u001b[39m top_10_tweets\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[0;32m      6\u001b[0m     display\u001b[38;5;241m.\u001b[39mclear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mH:\\Spark\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\sql\\session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39msql(sqlQuery, litArgs), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mH:\\Spark\\spark-3.4.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mH:\\Spark\\spark-3.4.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `tweets` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 23;\n'Project ['tag, 'count]\n+- 'UnresolvedRelation [tweets], [], false\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    time.sleep(3)\n",
    "    top_10_tweets = spark.sql('SELECT tag, count FROM tweets')  # Usando SparkSession\n",
    "    top_10_df = top_10_tweets.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=\"count\", y=\"tag\", data=top_10_df)\n",
    "    plt.show()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0082588-46a5-41d7-801c-2b940526bac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
